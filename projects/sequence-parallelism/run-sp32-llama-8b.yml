type: sft
micro_batch_size: 1
min_iterations: 300
exit_iteration: 300
activation_checkpoint_cpu_offload: true
tiled_mlp_compute: true

sequence_parallel_size: 32
deepspeed:
  zero_optimization:
    stage: 3
    offload_optimizer:
      device: cpu
  seq_parallel_communication_data_type: bf16

optimizer:
  type: cpu_adam
  learning_rate: 0.00001

model:
  type: "liger"
  name_or_path: meta-llama/Llama-3.1-8B-Instruct

  attn_implementation: flash_attention_2
  #attn_implementation: sdpa

data:
  sources:
    - HuggingFaceH4/ultrachat_200k
  #  - HuggingFaceH4/ultrachat_200k:train[:5000]
  cache_dir: /data-fast/data-cache/stas
  # num_proc: 16
  dl_num_workers: 1

#  max_length: 32_000 # short ok NV 78.49 GB
#  max_length: 1800000 # ok w/o PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True, but very close to OOM
#  max_length: 2200000 # ok w/ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
#  max_length: 12_000_000 # ok
#  max_length: 14_000_000 # short ok NV 74.51 GB
#  max_length: 15_000_000 # short ok NV 78.49 GB
  max_length: 15_500_000 # ? re-run w/ pt-nightly
#  max_length: 16_000_000 # oom

wandb:
#  enable: true
  enable: false
  project: "ulysses"
  name: "dp1-sp32-llama-8b"

# mem_profiler: e2e
# mem_profiler_max_entries: 1_000_000_000
# mem_profiler_dir: mem-prof-4n-8b

logger:
  level: WARNING
#  level: INFO

  output_dir: "logs"
  #file_output_ranks: [0,1]
  print_output_ranks: [0,1,2,3,4,5,6,7]
