type: sft
micro_batch_size: 1
exit_iteration: 1000
sequence_parallel_size: 8
deepspeed:
  steps_per_print: 1
  zero_optimization:
    stage: 3

model:
  # name_or_path: hf-internal-testing/tiny-random-LlamaForCausalLM
  # name_or_path: HuggingFaceTB/SmolLM-135M-Instruct
  name_or_path: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  #name_or_path: Felladrin/Llama-160M-Chat-v1
  # name_or_path: hfl/chinese-llama-2-1.3b
  #name_or_path: meta-llama/Llama-3.2-1B-Instruct
  # name_or_path: meta-llama/Llama-3.2-3B-Instruct
  # name_or_path: meta-llama/Llama-3.1-8B-Instruct
  #attn_implementation: eager
  # attn_implementation: flash_attention_2
  attn_implementation: sdpa

  # dtype: float32
  dtype: bf16

optimizer:
 learning_rate: 0.00001

#scheduler:
#  name: constant

data:
  sources:
    #- HuggingFaceH4/ultrachat_200k
    - HuggingFaceH4/ultrachat_200k-10k
  use_data_cache: true
  cache_processed_data: true
  # use_data_cache: false
  # cache_processed_data: false
  cache_dir: /data/data-cache/stas
  # num_proc: 16

#  max_length: 256
#  max_length: 512
  max_length: 1024
#  max_length: 2048
#  max_length: 8192
#  max_length: 16384
#  max_length: 32768
#  max_length: 65536
#  max_length: 98304
#  max_length: 131072

logger:
  level: WARNING
#  level: INFO

  output_dir: "logs"
  #file_output_ranks: [0,1]
  print_output_ranks: [0,1,2,3,4,5,6,7]


# checkpoint:
#   - type: huggingface
#     save_end_of_training: true
#     output_dir: ./fine-tuned-model


wandb:
#  enable: true
  enable: false
  project: "ulysses"
  name: "sp"


