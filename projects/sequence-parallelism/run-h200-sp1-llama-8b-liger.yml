type: sft
micro_batch_size: 1
exit_iteration: 30
min_iterations: 30
#activation_checkpoint_cpu_offload: true
#tiled_mlp_compute: true
sequence_parallel_size: 1

deepspeed:
  zero_optimization:
    stage: 3
    offload_optimizer:
      device: cpu
    offload_param:
      device: cpu

  seq_parallel_communication_data_type: bf16

optimizer:
  type: cpu_adam
  learning_rate: 0.00001

model:
  type: "liger"
  name_or_path: meta-llama/Llama-3.1-8B-Instruct

  attn_implementation: flash_attention_3
  #attn_implementation: sdpa

data:
  sources:
    - type: huggingface_instruct
      name_or_path: HuggingFaceH4/ultrachat_200k
      split: train_sft
      role_mapping:
        user: messages.role.user
        assistant: messages.role.assistant


  cache_dir: data-cache
  dl_num_workers: 1

  max_length: 300_000 # ok 134.96 GB

logger:
  level: WARNING
#  level: INFO

  output_dir: "logs"
  #file_output_ranks: [0,1]
  print_output_ranks: [0,1,2,3,4,5,6,7]


checkpoint:
  - type: huggingface
    save_end_of_training: true
    output_dir: ./fine-tuned-model
