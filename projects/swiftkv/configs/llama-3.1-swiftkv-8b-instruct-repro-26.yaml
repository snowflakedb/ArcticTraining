# From repro-23, fuse position ids for 20% of the batches
type: swiftkv
code: ../train.py
micro_batch_size: 1
epochs: 1
gradient_accumulation_steps: 1
sequence_parallel_size: 8
logits_loss_temp: 2.0
hidden_loss_layer: -2
model:
  name_or_path: meta-llama/Llama-3.1-8B-Instruct
  num_key_value_layers: 16
  key_value_group_size: 1
  attn_implementation: flash_attention_2
deepspeed:
  zero_optimization:
    stage: 2
data:
  sources:
    - type: nvidia/AceMath-Instruct-Training-Data
      name_or_path: nvidia/AceMath-Instruct-Training-Data
      split: general_sft_stage2
      sample_count: 500000
      kwargs:
        verification_mode: no_checks
    - type: lmsys/lmsys-chat-1m
      name_or_path: lmsys/lmsys-chat-1m
      sample_count: 500000
    - HuggingFaceH4/ultrachat_200k
    - Open-Orca/SlimOrca
    - THUDM/LongAlign-10k
    - Yukang/LongAlpaca-12k
    - type: ProjectGutenbergLong400K
      name_or_path: manu/project_gutenberg
      split: en
      sample_count: 600
    - type: ProjectGutenbergLong400K
      name_or_path: manu/project_gutenberg
      split: fr
      sample_count: 300
    - type: ProjectGutenbergLong400K
      name_or_path: manu/project_gutenberg
      split: de
      sample_count: 300
    - type: ProjectGutenbergLong400K
      name_or_path: manu/project_gutenberg
      split: es
      sample_count: 200
    - type: ProjectGutenbergLong400K
      name_or_path: manu/project_gutenberg
      split: nl
      sample_count: 200
    - type: ProjectGutenbergLong400K
      name_or_path: manu/project_gutenberg
      split: it
      sample_count: 160
    - type: ProjectGutenbergLong400K
      name_or_path: manu/project_gutenberg
      split: pt
      sample_count: 160
    - type: ProjectGutenbergLong400K
      name_or_path: manu/project_gutenberg
      split: zh
      sample_count: 80
  use_data_cache: true
  cache_processed_data: true
  cache_dir: /checkpoint/aqiao/swiftkv-long/data-cache
  num_proc: 16
  max_length: 131072
  always_max_length: true
  mask_inputs: true
  fuse_position_ids: true
  fuse_position_ids_prob: 0.2
logger:
  level: INFO
  output_dir: "./"
  file_output_ranks: [0]
scheduler:
  warmup_ratio: 0.05
optimizer:
  betas: [0.9,0.999]
  weight_decay: 0.0
  lr: 0.0002
checkpoint:
  - type: huggingface
    save_every_n_epochs: 1
    output_dir: /checkpoint/aqiao/swiftkv-long/llama-3.1-swiftkv-8b-instruct-repro-26
    save_end_of_training: true
