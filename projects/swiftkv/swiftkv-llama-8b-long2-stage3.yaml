type: swiftkv
code: train_llama.py
micro_batch_size: 1
epochs: 1
gradient_accumulation_steps: 1
temperature: 2.0
decoder_loss_mult: 0.0
sequence_parallel_size: 8
model:
  name_or_path: /checkpoint/aqiao/icml25/swiftkv-llama-8b-long2-stage2/global_step_12580/
  num_key_value_layers: 16
  key_value_group_size: 1
  attn_implementation: sdpa
deepspeed:
  zero_optimization:
    stage: 3
data:
  sources:
    - THUDM/LongAlign-10k
    - Yukang/LongAlpaca-12k
  use_data_cache: false
  cache_processed_data: true
  cache_dir: /data-fast/data-cache
  num_proc: 16
  max_length: 100000
  mask_inputs: false
logger:
  level: INFO
  #output_dir: "./"
  #file_output_ranks: [0]
scheduler:
  warmup_ratio: 0.05
optimizer:
  betas: [0.9,0.999]
  weight_decay: 0.0
  lr: 0.0002
checkpoint:
  - type: huggingface
    save_every_n_steps: 1000000
    output_dir: /checkpoint/aqiao/icml25/swiftkv-llama-8b-long2-stage3
    save_end_of_training: true
