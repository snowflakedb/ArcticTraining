type: dpo
micro_batch_size: 1
epochs: 1
gradient_accumulation_steps: 1
beta: 0.1
model:
  type: liger
  dtype: float32
  name_or_path: Qwen/Qwen2.5-0.5B-Instruct
  # attn_implementation: flash_attention_2
ref_model:
  type: liger
  name_or_path: Qwen/Qwen2.5-0.5B-Instruct
  # attn_implementation: flash_attention_2
deepspeed:
  zero_optimization:
    stage: 2
wandb:
  enable: true
  project: arctic-dpo-loss-sml
  name: arctic-dpo-sml
data:
  sources:
    - HuggingFaceH4/ultrafeedback_binarized
  use_data_cache: true
  cache_processed_data: true
  cache_dir: /data-fast/data-cache
  num_proc: 16
  max_length: 8192
logger:
  level: WARNING
  output_dir: "./"
  file_output_ranks: [0]
scheduler:
  name: 'constant'
  warmup_ratio: 0.0
optimizer:
  betas: [0.9,0.999]
  weight_decay: 0.1
  lr: 1e-6
checkpoint:
  - type: huggingface
    save_every_n_steps: 10000
    output_dir: /data/dpo-llama-8b
    save_end_of_training: true
