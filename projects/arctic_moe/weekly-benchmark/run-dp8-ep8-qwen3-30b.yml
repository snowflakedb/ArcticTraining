type: sft
micro_batch_size: 1
min_iterations: 12
exit_iteration: 12

arctic_moe: true
expert_parallel_size: 8

deepspeed:
  zero_optimization:
    stage: 2
    # offload_optimizer:
    #   device: cpu

optimizer:
  type: fused_adam_moe
  learning_rate: 0.00001

model:
  type: "liger"
  name_or_path: Qwen/Qwen3-30B-A3B-Instruct-2507
  #name_or_path: snake7gun/tiny-random-qwen3moe
  #name_or_path: DavidAU/Qwen3-MOE-4x0.6B-2.4B-Writing-Thunder
  #name_or_path: DavidAU/Qwen3-MOE-6x1.7B-10.2B-Shining-Madness-Uncensored
  # hf_config_kwargs:
  #   #num_hidden_layers: 48 #
  #   num_hidden_layers: 24 #

  attn_implementation: flash_attention_2
  #attn_implementation: sdpa

data:
  sources:
  #  - HuggingFaceH4/ultrachat_200k
    - HuggingFaceH4/ultrachat_200k:train[:10000]
  cache_dir: /data-fast/data-cache/stas
  # num_proc: 16
  dl_num_workers: 1

# how we calculate the equivalent seqlen against dense baseline when using MoE
# Qwen/Qwen3-30B-A3B-Instruct-2507
# equivalent_moe_seqlen = dense_seqlen * (num_experts / num_experts_per_tok) / n_gpus
#
# num_experts=128
# num_experts_per_tok=8
# dense_seqlen=16_000
# n_gpus=8
# equivalent_moe_seqlen = 32_000

# see the math above when choosing max_length
  max_length: 32_000 # => 16k in the dense equivalent because of the num of experts mapping

wandb:
  enable: true
  #enable: false
  project: arctic-moe
  name: dp8-ep8-amoe-z2-Qwen3-30B

logger:
  level: WARNING
#  level: INFO

  output_dir: "logs"
  #file_output_ranks: [0,1]
  print_output_ranks: [0,1,2,3,4,5,6,7]
